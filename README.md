# SampledWandbLogger

Provides a class: `sampled_log.SampledWandbLogger`, that downsamples calls to `wandb.log()` while ensuring we save a permanent record of all calls.

This class can be used to improve wandb frontend performance, by limiting the size
of data saved to the wandb metrics store via wandb.log. You'll only see improvements if you're already calling wandb.log() millions of times in your jobs.

In addition to downsampling wandb.log() calls, all wandb.log() calls all stored in a chunked parquet table and sync'd as an Artifact. You can use W&B Artifacts APIs to grab the entire dataset later if you need access to unsampled metrics.

This is designed to be future compatible. We are working on a faster and more scalable metrics store implementation built on top of chunked parquet files. In the future you will be able to use the W&B UI to plot metrics stored in the parquet tables generated by SampledWandbLogger.

VERY IMPORTANT CAVEAT: This class downsamples by dropping data points at the target rate. You should only use it if you have a single set of keys that you pass to every wandb.log() call in your program. For example, using SampledWandbLogger will produce unwanted effects if you log a set of metrics on each training step, and a different set of metrics at the end of each epoch. All wandb.log() calls are sampled at the same rate, even for sets of metrics whose relative logging frequency is low. This is probably not the behavior you want in that case.

CAVEAT #2: You must call run.finish() explicitly when using SampledWandbLogger!

## Setup

```
pip install wandb fastparquet pandas
```

## Running the example

```
wandb login
python example.py
```
